---
layout: mermaid
classes: wide
title: "coxwave 기술과제"
subtitle: "아파트에서 문을 열어놓는 것의 의미"
date: 2024-05-08
categories: 개발이야기
published: false
---

## Submission #1 Tech Spec

### Mission

Read the project background below and write the Tech Spec for the AI Proxy.

Project Background

>Align AI utilizes OpenAI's GTP-3.5 model(LLM) across various parts of its products. Until now, individual services that use
 the model have directly called and used the OpenAI Service. However, as demand for the service increases, we're facing
 the following problems:
>
>* Inefficiency of having to repeatedly implement Rate Limit and Retry Backoff
>* As usage increases, quota is insufficient with a single provider, resulting in the need to use multiple providers
>(ex - leveraging multiple regions of Azure OpenAI Service)
>* Inconvenience in managing API Keys for each environment and service
>* Difficulties in LLM Usage Analysis
>
>To address these issues, we aim to develop "AI Proxy".

---

## 용어 정리

* service: 일반적으로 자사 제품 내부에서 특정 기능을 제공하는 유무형의 단위
  * OpenAI Service: OpenAI에서 제공하는 GTP-3.5 model을 이용하는 서비스
* Rate Limit and Backoff: ``자사의 서비스에서 OpenAI 서비스``에 요청을 보낼 때,
    요청을 제한하는 정책과 요청이 실패했을 때 재시도하는 정책
* LLM Usage Analysis: ``OpenAI 서비스``를 사용하는 사용량 분석

---

## Description of the Problem

> Give a brief (one paragraph) summary of the problem you are solving. Work to keep away from technical detail. Try instead to talk about this in terms of the problem as it pertains to your customers. At the end of reading this document, any team member should be able to understand the problem, how you intend to solve it, and who are the stakeholders.

### My Description of the Problem

> 우리 인공지능 서비스에 대한 수요가 증가하고 있어서, 사람이 몰렸을 때 서비스가 느려지고 에러가 발생해 사용자
> 경험이 떨어지는 문제가 발생하고 있습니다. 이에 대응하는 방법으로 여러 공급자를 이용해서 제품의 확장성을
> 높이고자 합니다. 이 때 개발과정에서 통합된 시스템이 갖춰지지 않을 경우 개발 효율성이 떨어질 수 있습니다.
> 이를 해결하기 위한 통합 시스템으로 AI Proxy 개발을 제안합니다. AI Proxy는 공급자측 서버 요청 거부 및 실패에
> 대응하거나, 공급자 별로 요청을 분배하고, API를 요청하는 데 필요한 키를 관리하며, 사용량을 분석하는
> 기능을 가지는 모듈로 구성된 통합 시스템으로 개발 예상 기간은 약 2개월입니다.

---

## Solution Requirements

> Briefly describe what is required of a solution which addresses the problem. Try to steer clear of the how (implementation detail) and concentrate on what is required any solution in order to address the problem outlined above.

### My Solution Requirements

> 솔루션은 다음과 같은 요구사항을 충족해야합니다.
>
> 1. DRY(Don't Repeat Yourself) 원칙을 준수하기 위해 Rate Limit을 implement하고 Backoff를 재시도하는 부분을 별도의 모듈로 분리해야
> 합니다. 이때 해당 모듈은 Rate limit과 Backoff의 정책(soft limit, hard limit, retry count, retry interval 등)을 변경할 수 있어야
> 합니다.
> 2. 다양한 지역의 Azure OpenAI Service를 사용하면서 생기는 응답의 delay간의 차이를 최소화하기 위해 provider별 응답 시간을 측정하고,
> 오랜 시간이 걸려도 되는 요청과 빠른 응답이 필요한 요청을 분리하는 load balancer를 구현해야 합니다.
> 3. API Key를 관리하기 쉽도록 모든 환경과 서비스에 대한 API Key를 한 곳에서 관리할 수 있어야 합니다. docker-compose나 k8s와 같은
> 솔루션을 사용해도 되며, 환경변수를 통해 API Key를 관리할 수 있어야 합니다.
> 4. LLM 사용 분석을 위해 load test를 구성 및 수행하고, 사용량을 모니터링할 수 있어야 합니다. load test에 사용할 수 있는 툴에는
> grafana/k6, locust, jmeter 등이 있습니다.

---

## Out-of-scope (Non-goals)

> Explicitly call out what is not in scope (Sometimes articulating what you are not going to do is an easier way to define scope than to talk about what you are going to do.)

### My Out-of-scope

> 솔루션은 다음과 같은 부분을 포함하지 않습니다.
>
> 1. Azure OpenAI Service를 여러 지역에서 사용하는 것을 넘어, 다른 vendor를 사용했을 때 발생할 구현적인 문제는 포함하지 않습니다.
> 2. 고객의 수요는 이미 예측 가능하다 가정하고 이 수요에 따른 tps를 측정하는 LLM Usage Analysis를 하며, 고객의 수요 자체 예측을 위한
> 솔루션은 포함하지 않습니다. 대신 AI Proxy의 load test 모듈에서 발생하는 tps를 모니터링한 뒤 이에 맞춘 환경을 구성하는 것은
> 포함합니다.
> 3. Rate Limit과 Backoff를 비즈니스 로직에 맞추어 구현하는 방법에 대한 구체적인 내용은 포함하지 않습니다. 대신 이를 반복적으로
> 구현하지 않도록 모듈화하는 부분에 대해서만 언급합니다.(나중에 제외 고려)

---

## Glossary

> Define any new terms used in this document, if needed.

### My Glossary

> 1. Rate Limit
> : 서버가 특정 임계치까지만 클라이언트의 요청을 허용하는 정책을 의미한다. 서버에서 제공할 수 있는 자원에는 제한이 있기 때문에
> 안정적으로 서비스를 제공하기 위해 사용되는 대표적인 `혼잡 제어 기법`으로, 예를 들어 서버에 호출 제한 횟수를 분당 60회로 제한하고,
> 이를 넘어설 경우 요청을 처리하지 않고 에러(429 Too Many Requests)를 반환하는 throttle을 구현하는 것이다.
>
> 2. Backoff
> : 서버에 요청을 보낸 후 응답이 오지 않거나 실패했을 때, 일정 시간이 지난 후 다시 요청을 보내는 정책을 의미한다. 서버의 부하를
> 줄이기 위해 사용되며, 서버에 요청을 보낸 후 시간이 증가하는 형태가 지수적으로 증가하는 Exponential Backoff과, 일정 범위 내에서
> 랜덤한 시간을 추가하는 Jitter Backoff이 있으며, 이 두 가지를 혼합한 방법도 있다.

---

## Solution

> Your solution goes here.
If you're not sure about some parts, list out everything you're considering and write it down, so that reviewers can help
 you make the right decision. It’s good to include diagram about how your solution interacts with other systems/services. (It can be sequence diagrams for complex interactions, database ERD, and so on.)
> Aim to answer:
>
> * What are you going to deliver?
> * What are your dependencies?
> * What are the limits of your solution?
> * How might your solution evolve to meet future requirements?
> * (Backend/Infra) How will it scale?
> * (Backend/Infra) How will you ensure fault tolerance and quick recovery after failure?

### What are you going to deliver?

솔루션은 다음과 같은 것을 제공합니다.

> 1. Rate Limit과 Backoff을 구현하는 모듈
> 2. 다양한 지역의 Azure OpenAI Service를 사용하면서 생기는 응답의 delay간의 차이를 최소화하기 위한 load balancer
> 3. docker-compose 또는 k8s를 사용해 API Key를 관리할 수 있는 환경
> 4. LLM Usage Analysis를 위한 load test 구성 및 수행
> 5. 사용량 모니터링을 위한 grafana 대시보드
> 6. 솔루션의 모든 부분을 통합하는 AI Proxy
>
> 여러 서비스에서 동일한 기능에 대해 `통합 모듈`을 사용하는 것은 특히나 Rate Limit과 Backoff과 같은 반복적인
> 구현을 줄이는 데 큰 도움이 됩니다. 이를 통해 개발자는 Rate Limit과 Backoff을 구현하는 데 드는 시간을 줄이고,
> 이를 통해 개발 효율성을 높일 수 있습니다. 하지만 `통합 모듈`을 사용하는 데 있어 기존의 서비스들이 Rate Limit과
> Backoff을 사용하던 방식을 변경해야 하는 경우를 최소화해야 합니다. 예를 들어 기존에
> Rate Limit과 Backoff를 구현은 각 서비스 별로 다른 정책과 설계를 가지고 있을 수 있기 때문에 이를 고려하지 않으면
> 기존의 서비스들에 대한 변경이 많아져 기존의 목적인 개발 효율성 향상을 달성하기 어려울 수 있습니다.
>
> 이를 해소하기 위해 우선적으로 각각의 서비스에 대한 호환성이 보장되는 `통합 모듈`을 구현해야 합니다. 해당 통합 모듈은
> 높은 유연성을 가지고 있어야 하며, 이를 통해 기존의 서비스들이 Rate Limit과 Backoff을 이용하는 방식을 적게
> 변경하고도 새로운 `통합 모듈`을 사용할 수 있어야 합니다. 해당 모듈을 통한 이전이 확실히 이루어진 것을 확인한
> 다음에는 기존의 서비스들이 Rate Limit과 Backoff을 사용하던 방식을 분리 및 삭제하는 작업을 진행해야 합니다.
>
> `Load Balancer`는 규모를 확장시키기 위한 방법 중 주로 사용되는 방법 중 하나로, 요청을 분산하는 역할을 통해
> 서비스의 확장성을 높일 수 있습니다. 특히나 현재 Align AI에서 사용하고 있는 Azure OpenAI Service를 여러 지역에서
> 사용하는 데 있어서 각각이 개별적으로 요청을 보내는 설계로 인해 특정 지역의 서비스가 과부하가 걸릴 수 있는 문제가
> 있습니다. 이를 해결하기 위해 AI Proxy를 통해 통합된 `Load Balancer`를 구현하고 요청을 분산할 경우, 각각의 서비스가
> 과부하가 걸리지 않도록 요청을 분산하고 서비스의 확장성을 높일 수 있습니다.
>
> `Load Balancer`는 요청을 받아들이고, 요청을 분류하며, 분류된 요청을 올바른 LLM 서비스로 보내는 역할을 합니다.
> 요청을 받아들이는 부분은 REST API로 구현할 수 있으며, 서비스에서 요구하는 우선순위에 따라 요청을 분류해야
> 합니다. 우선순위의 종류에는 요청의 크기, 요청자의 우선순위 등이 있을 수 있습니다. 분류된 요청을 올바른 LLM
> 서비스로 보내는 부분은 Rate Limit과 Backoff을 구현하는 모듈과 연동되어야 합니다. 요청이 실패했을 때, Rate
> Limit과 Backoff에 따른 재시도 및 실패 처리가 올바르게 이루어지는지 확인해야 합니다.
>
> `API Key Management Environment`는 API Key를 환경 변수로 관리할 수 있도록 구성합니다. 이를 위해 사용 예정인
> 솔루션은 `docker-compose`입니다. docker-compose는 여러 컨테이너를 한 번에 실행할 수 있도록 하는 도구로,
> 여러 컨테이너를 실행할 때 환경 변수를 설정할 수 있습니다. 이 때문에 추후에 orchestration을 통한 인프라 확장을
> 해야하는 경우에도 환경 변수를 통해 API Key를 관리할 수 있는 장점이 있습니다.
>
> `Load Test`는 다양한 부하에 대한 테스트를 수행하는 도구입니다. `Load Test`를
> 사용해 LLM 사용량을 분석할 경우 현재 구현 예정인 API Proxy의 load balancer가 요청 분산을 통해 얻은 성능 향상
> 이나 Rate Limit과 Backoff의 정책 변경에 따른 성능 변화를 확인할 수 있습니다. 또한 이외에 다른 확장성을
> 확보하기 위한 솔루션이나 새로운 공급자를 사용할 때의 성능 변화 등을 확인을 용이하게 해줍니다. 이를 위한 도구로는
> k6, locust, jmeter 등이 있는데, 이 중에서 go로 구현돼 있어 성능이 우수하다는 평가를 받고 있고 제가 사용해본
> 경험도 있는 `k6`를 솔루션으로 사용할 예정입니다.
>
> `Load Test`는 QPS를 기준으로 LLM Usage Analysis를 할 예정입니다. 내부 서비스에서 AI Proxy를 통해 보낸
> 요청의 QPS를 측정하고 분석하면 LLM Usage Analysis를 할 수 있습니다. 이를 통해 예상되는 이용자들의 요청량 변화에
> 따른 Align AI 서비스의 부하 처리 능력을 예측할 수 있습니다. 해당 테스트는 가급적 실제 서비스 환경과 유사하게
> 구성하되, 실물 OpenAI Service를 사용하지 않고 mock LLM 서비스를 사용할 예정입니다. 이는 비용을 줄이는 동시에
> 자사 서비스에 영향을 주지 않고 테스트를 할 수 있기 때문입니다.
>
> `모니터링`은 추가적인 확장성 확보를 위한 방법 중 하나로, 사용자의 요청에 따른 서비스의 부하를 확인할 수 있습니다.
> 이를 통해 기존 서비스의 부하 처리 능력을 개선해야하는 시점 및 부분을 파악할 수 있습니다. 프로그램의 성능은 20%의
> 부분이 80%의 성능을 결정한다는 80/20 법칙에 따라, `모니터링`을 통해 부하가 발생하는 부분을 파악하면 프로그램의
> 성능을 개선하는 데 큰 도움이 될 수 있습니다.
>
> 또한 k8s와 같은 솔루션을 사용해 자동으로 규모를 확장하게 하는 등의 솔루션을 추후에 적용하는 것은 확장성을 확보하는
> 좋은 방법 중 하나입니다. 이 때 해당 확장을 진행하는 여부에 대한 기준을 `모니터링` 결과가 있다면 수월하게 결정할
> 수 있을 것입니다.
>
> 여러 `모니터링` 솔루션 중 `grafana`는 널리 쓰이는 오픈소스 `모니터링` 도구로, 특히 load test로 사용하는 k6와 연동
> 측면에서도 강점을 가지고 있기 때문에 사용할 예정입니다.
>
> 최종적으로는 이들을 통합하는 `AI Proxy`를 개발할 예정입니다. `AI Proxy`는 Rate Limit과 Backoff을 구현하는 모듈, Load
> Balancer, API Key Management Environment, Load Test, Grafana Dashboard를 통합하는 역할을 합니다. 기존에
> 서비스들은 이들을 각각 구현하고 있었지만, 이를 통합함으로써 체계적인 시스템을 통해 효율적인 개발 및 확장성을
> 확보할 수 있을 것으로 기대합니다.

### What are your dependencies?

![Algin AI 관계도](/images/휴지통/align%20ai%20관계도.png)

> AI Proxy는 monitoring system, load balancer와 load test 세 가지 하위 모듈에 의존하고 있습니다. 이들은
> 현재 AI Proxy의 Spec으로 Azure OpenAI Provider에 보내기 위한 요청 처리 및 분석을 담당하고 있습니다. 각각의
> 모듈은 각각 별개의 역할인 부하의 분산, 요청의 감시 및 분석, 수요에 따른 부하 처리 능력을 확인하는 기능을
> 수행하고 있으며, AI Proxy는 이들을 통합하는 과정에서 의존성을 띠게 됩니다.
>
> load balancer는 두 가지 형태의 의존성을 가지고 있습니다. 첫 번째는 load balancer의 내부 모듈인 API Key
> Management Environment와 Rate Limit과 Backoff을 구현하는 모듈에 대한 의존성입니다. 이들은 load balancer의
> 핵심적인 기능을 수행하는 모듈이기 때문에, load balancer가 이들을 통해 요청을 분류하고, 분류된 요청을 올바른
> LLM 서비스로 보내는 역할을 수행할 수 있습니다.
>
> 두 번째는 외부 서비스인 Azure OpenAI Provider에 대한 의존성입니다. Azure OpenAI Provider는 AI Proxy의
> 요청을 받아들이고, 이를 처리한 뒤 결과를 반환하는 역할을 수행합니다. 이는 AI Proxy가 요청을 보내는 대상이기
> 때문에, Azure OpenAI Provider가 다운되거나 정책의 문제로 인해 서비스 운영이 불가능해질 경우 AI Proxy의
> 서비스 운영이 중단될 수 있습니다. 따라서 이를 고려한 대안을 마련해야 합니다.

### What are the limits of your solution?

AI Proxy의 솔루션은 다음과 같은 제한을 가집니다.

> 1. AI Proxy가 하위 모듈에 의존하고 있기 때문에, 하위 모듈 구현을 변경하면 AI Proxy도 변경해야 합니다. 하위 모듈들의 정책은 자주 변경 가능한 항목이기 때문에, AI Proxy가 이를 반영할 수 있도록 설계되어야 합니다.
> 2. Load Balancer가 weakest link가 되어, 이를 통해 들어오는 요청을 처리하는 속도가 느려지면 AI Proxy의 성능이 저하될 수 있습니다.
> 3. 단일 vendor에게 의존하고 있기 때문에 해당 vendor가 다운되거나, 정책의 문제로 인해 [서비스 운영이 불가능](https://kaestro.github.io/weeklyposts/2024/04/04/Post-reviews.html#%EB%B9%8C%EB%A6%B0-%EB%95%85-%EC%9C%84%EC%97%90-%EC%84%B1%EC%9D%84-%EC%A7%93%EC%A7%80-%EB%A7%90%EB%9D%BC-%ED%95%98%EB%A3%A8%EB%A7%8C%EC%97%90-4%EC%B2%9C%EB%A7%8C-%EB%8B%AC%EB%9F%AC%EA%B0%80-%EC%A6%9D%EB%B0%9C%ED%95%9C-%EC%9D%B4%EC%9C%A0)해질 수 있습니다.
> 4. AI Proxy 자체가 다운되는 경우, Align AI의 서비스가 중단될 수 있습니다.

### How might your solution evolve to meet future requirements?

AI Proxy의 솔루션은 다음과 같은 방향으로 발전할 수 있습니다.

> 1. 의존성 주입 혹은 역전 제어를 사용해 AI Proxy가 하위 모듈의 구현을 변경하지 않고도 새로운 모듈을 추가할 수 있도록 확장할 수
> 있습니다.
> 2. Load Balancer의 성능을 개선하거나, 인프라가 부하에 맞추어 자동으로 확장되도록 하는 방법을 고려할 수 있습니다.
> 3. 여러 vendor를 사용하도록 확장하거나, 다른 vendor로 전환할 수 있도록 하거나, 스스로 오픈 소스 LLM(llama)등을 이용해 스스로
> 요청을 처리하는 비즈니스 로직을 구현할 수 있습니다.
> 4. AI Proxy 자체에 장애가 발생할 경우를 대비해 기존에 사용하던 요청 서버를 백업으로 두거나, k8s와 같은 솔루션을 사용해 자동으로
> 복구할 수 있도록 할 수 있습니다.

### (Backend/Infra) How will it scale?

> 1. MSA를 통해 load balancer의 모듈을 별도의 서비스로 분리하고, 이를 여러 인스턴스로 확장할 수 있습니다.
> 이 경우에는 Rate Limit과 Backoff을 구현하는 모듈을 별도의 서비스로 분리할 수 있습니다.
> 2. 환경 변수를 서버의 메모리와 저장 장치가 아닌 별도의 캐시 서버에 저장하고, 이를 통해 API Key를 관리할 수
> 있습니다.
> 3. Load Test를 수행하는 서버를 k8s와 같은 솔루션을 사용해 자동으로 확장할 수 있습니다.

### (Backend/Infra) How will you ensure fault tolerance and quick recovery after failure?

> 1. load balancer를 단일 서버로 구성하지 않고, 여러 인스턴스로 구성하여 단일 서버의 장애에 대비할 수 있습니다.
> 2. [Chaos Monkey](https://www.techtarget.com/whatis/definition/Chaos-Monkey)와 같은 도구를 사용해 서비스의 장애를 시뮬레이션하고, 이를 통해 장애에 대한 대응 방안을
> 찾을 수 있습니다.

---

## Risks and Open Issues

> If there are any risks or unknowns, list them here.
If this project is public-facing, you must list security risks too.

### Expected Risks

> 1. 공급자 다각화에 따른 서비스 운영의 비용 증가
> 2. 시스템 통합에 따른 복잡성 및 호환성 문제에 따른 오류 발생

### Open Issues

> 1. 사용자 증가에 따른 서버 부하에 대한 대응
> 2. 통합된 시스템 구축을 통한 개발 효율성 향상

---

## Work Required

> Include a high level breakdown of the work required to implement your proposed solution.
It’s good to include rough milestone (task duration, expect date to complete).

### My Work Required

> 1. Rate Limit과 Backoff을 구현하는 모듈
> : 기존의 Align AI 서비스에서 사용하던 Rate Limit과 Backoff을 분리하고, 이를 모듈화하는 작업을 진행합니다.
> : 예상 소요기간 - 1달
> : 1 ~ 3일 - 각각의 서비스에서 사용하던 Rate Limit과 Backoff을 분석하고 정리한 뒤 통합 모듈을 설계
> : 4 ~ 9일 - 서비스 별 정책을 적용할 수 있는 호환성 높은 통합 모듈을 구현
> : 10 ~ 12일 - 서비스들에서 기존의 Rate Limit과 Backoff을 사용하던 방식을 변경하지 않고도 새로운 모듈을 사용할 수 있는지 확인
> : 13 ~ 15일 - 결과 확인 및 오류 수정
> : 16 ~ 18일 - 기존의 서비스들이 Rate Limit과 Backoff을 사용하던 방식을 분리 및 삭제
> : 19 ~ 21일 - 결과 확인 및 오류 수정
> 2. Key Management Environment 구성
> : docker-compose를 통해 API Key를 환경 변수로 관리할 수 있도록 구성합니다.
> : 예상 소요기간 - 5일
> : 1 ~ 2일 - docker-compose를 통해 API Key를 환경 변수로 관리하는 방법을 찾아 학습
> : 3일 - docker-compose를 통해 API Key를 환경 변수로 관리하는 환경을 구성
> : 4 ~ 5일 - 환경이 올바르게 구성되었는지 확인 및 오류 수정
> 3. Load Balancer 구현
> : 요청을 받아들이고, 요청을 분류하며, 분류된 요청을 올바른 LLM 서비스로 보내는 load balancer를 구현합니다.
> : 예상 소요기간 - 2.5주
> : 1 ~ 3일 - 요청을 받아들이는 부분 구현
> : 4 ~ 6일 - 요청을 분류하는 부분 구현
> : 7 ~ 10일 - 하위 모듈과 연동하여 분류된 요청을 올바른 LLM 서비스로 보내는 부분 구현
> : 11 ~ 14일 - 하위 모듈과 연동하여 요청이 실패했을 때, 이에 대한 처리하는 부분 구현
> : 15 ~ 18일 - 결과 확인 및 오류 수정
> 4. LLM Usage Analysis를 위한 load test 구성 및 수행
> : load test를 구성하고, 수행하여 LLM 사용량을 분석합니다.
> : 예상 소요기간 - 3일
> : 1일 - load test 시나리오 작성
> : 2일 - k6를 통해 load test를 수행 및 동작 확인
> : 3 ~ 5일 - 결과 분석 및 오류 수정
> 5. 사용량 모니터링을 위한 grafana 대시보드 구성
> : grafana 대시보드를 구성하여 사용량을 모니터링합니다.
> : 예상 소요기간 - 1주
> : 1 ~ 4일 - grafana 대시보드 사용법 학습 및 환경 구성
> : 5 ~ 7일 - 사용량 모니터링 결과 확인 및 오류 수정

---

## High-level Test Plan

> At a high level, describe how your chosen solution be tested.
It’s good to think about metrics that can validate this solution, and checklists for QA.

### My High-level Test Plan

> 가정
> : 항상 일정한 시간에 request에 대한 답변을 보내는 시뮬레이션이 돼어있는 mock LLM 서비스가 있고, 테스트
> 환경에서는 이 mock LLM 서비스를 사용한다 가정하겠습니다. 이 mock LLM 서비스는 loadbalancer를 통해 요청을
> 받아들인 뒤 일정한 시간에 응답을 보내며 이때 응답 시간 조절을 통해 응답 시간의 차이를 발생시킬 수 있습니다.
>
> 또한 mock LLM 서비스는 실패에 대한 시뮬레이션을 위해, 일정한 확률로 실패(http response 500)하는 결과를 반환할 수 있습니다.
> 해당 실패하는 확률은 임의로 설정할 수 있습니다.
>
> 1. Rate Limit과 Backoff을 구현하는 통합 모듈에 대한 호환성 테스트
> : Rate Limit과 Backoff 기능은 기존의 서비스들이 사용하던 Rate Limit과 Backoff을 대체하는 역할을 합니다.
> 따라서 기존의 서비스들이 새로운 모듈을 사용했을 때, 기존의 Rate Limit과 Backoff을 사용하던 방식을 가능한
> 변경하지 않고도 새로운 모듈을 사용할 수 있는지 확인합니다.
> : 또한 각각의 서비스들이 적용중이던 정책 및 세부사항이 다를 가능성이 높기 때문에, 이를 모듈화하는 과정에서
> customizing이 가능한 형태로 구현하고 이를 통해 호환성을 확인합니다. 호환이 불가능한 형태로 구현되어 있다면,
> 기존의 서비스들이 Rate Limit과 Backoff를 사용하는 방식을 더 유연하게 변경할 수 있도록 수정하는 작업을
> 선행합니다.
> 2. Load Balancer에 대한 테스트
> : Load Balancer가 요청을 받아들이는지 확인합니다.
> : 받은 요청을 서비스에서 요구하는 우선순위(ex. 사용자의 요금제)에 따라 분류하는지 확인합니다.
> : 분류된 요청을 그에 맞는 LLM 서비스 공급자에게 보내는지 확인합니다.
> : 요청이 실패했을 때, Rate Limit과 Backoff에 따른 재시도 및 실패 처리가 올바르게 이루어지는지 확인합니다.
> 3. API Key Management Environment에 대한 테스트
> : API Key가 환경변수로 관리되는지 확인합니다.
> : API Key가 올바르게 설정되었을 때, 요청이 올바른 LLM 서비스로 보내지는지 확인합니다.
> : API Key가 잘못 설정되었을 때, 요청이 올바른 LLM 서비스로 보내지지 않는지 확인합니다.
> 4. grafana 대시보드를 통해 사용량 변화에 따른 LLM Usage Analysis 모니터링을 위한 load test
> : 점진적으로 Mock LLM service Provier에 대한 부하를 두 가지 형태로 증가시킵니다. 그 중 첫 번째는 QPS를
> 증가시키는 방법이며, 두 번째는 응답 거부율을 증가시키는 방법입니다. 해당 부하에 대한 응답을 대시보드를
> 통해 확인하고, 이에 따른 Rate Limit과 Backoff의 정책을 변경이 정상적으로 이루어지는지, 그리고 이에 따른
> 성능 변화가 어떻게 나타나는지 확인합니다.

---

## References / Appendix

> Links to any other documents that may be relevant, or sources you wish to cite.

* [Rate Limit이란?](https://etloveguitar.tistory.com/126)
* [Retry 전략에 대해서(Exponential Backoff, jitter)](https://jungseob86.tistory.com/12)
* [Back-off Algorithm for CSMA/CD](https://www.geeksforgeeks.org/back-off-algorithm-csmacd/)
* [빌린 땅 위에 성을 짓지 말라](https://kaestro.github.io/weeklyposts/2024/04/04/Post-reviews.html#%EB%B9%8C%EB%A6%B0-%EB%95%85-%EC%9C%84%EC%97%90-%EC%84%B1%EC%9D%84-%EC%A7%93%EC%A7%80-%EB%A7%90%EB%9D%BC-%ED%95%98%EB%A3%A8%EB%A7%8C%EC%97%90-4%EC%B2%9C%EB%A7%8C-%EB%8B%AC%EB%9F%AC%EA%B0%80-%EC%A6%9D%EB%B0%9C%ED%95%9C-%EC%9D%B4%EC%9C%A0)
* [Scaling Backend Systems](https://tagwizz.com/blog/seven-ways-to-improve-scalability-in-backend-systems)
* [Chaos Monkey](https://www.techtarget.com/whatis/definition/Chaos-Monkey)
* [difference between risk & issue](https://www.knowledgehut.com/blog/project-management/what-is-the-difference-between-risk-issues)
